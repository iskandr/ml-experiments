{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max MHC seq len: 403\n"
     ]
    }
   ],
   "source": [
    "from pepdata.amino_acid_alphabet import canonical_amino_acid_letters\n",
    "\n",
    "\n",
    "filename=\"class1_sequences.csv\"\n",
    "allow_unknown_amino_acids=True\n",
    "add_start_token=True\n",
    "add_stop_token=True\n",
    "include_tap_genes = False\n",
    "\n",
    "df = pd.read_csv(filename)\n",
    "if not include_tap_genes:\n",
    "    tap_mask = df.gene.str.startswith(\"TAP\")\n",
    "    df = df[~tap_mask]\n",
    "    \n",
    "mhc_sequences = df.seq\n",
    "labels = df.species_prefix\n",
    "\n",
    "token_order = {t: i + 1 for i, t in enumerate(canonical_amino_acid_letters)}\n",
    "token_order[\"-\"] = 0\n",
    "n_tokens = 21\n",
    "\n",
    "if allow_unknown_amino_acids:\n",
    "    mhc_sequences = mhc_sequences.str.replace(\"?\", \"X\")\n",
    "    token_order[\"X\"] = n_tokens\n",
    "    n_tokens += 1\n",
    "\n",
    "if add_start_token:\n",
    "    mhc_sequences = \"^\" + mhc_sequences\n",
    "    token_order[\"^\"] = n_tokens\n",
    "    n_tokens += 1\n",
    "\n",
    "if add_stop_token:\n",
    "    mhc_sequences = mhc_sequences + \"$\"\n",
    "    token_order[\"$\"] = n_tokens\n",
    "    n_tokens += 1\n",
    "\n",
    "\n",
    "max_seq_len = mhc_sequences.str.len().max()\n",
    "print(\"Max MHC seq len: %d\" % max_seq_len)\n",
    "def pad_seq(s):\n",
    "    n_padded_chars_needed = max_seq_len - len(s)\n",
    "    result = s + (\"-\" *  n_padded_chars_needed) \n",
    "    assert len(result) == max_seq_len\n",
    "    return result\n",
    "                  \n",
    "padded_mhc_sequences = mhc_sequences.map(pad_seq)\n",
    "n_samples = len(padded_mhc_sequences)\n",
    "encoded_sequences = np.zeros(\n",
    "    (n_samples, max_seq_len), dtype=\"int32\")\n",
    "for i, s in enumerate(padded_mhc_sequences):\n",
    "    encoded_sequences[i, :] = [token_order[si] for si in s]\n",
    "\n",
    "unique_labels = list(set(labels.unique()))\n",
    "label_order = {s: i for (i, s) in enumerate(unique_labels)}\n",
    "encoded_labels = labels.map(lambda s: label_order[s]).values\n",
    "\n",
    "\n",
    "n_labels = len(label_order)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species_prefix</th>\n",
       "      <th>gene</th>\n",
       "      <th>allele</th>\n",
       "      <th>seq</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14267</th>\n",
       "      <td>SLA</td>\n",
       "      <td>7</td>\n",
       "      <td>02:01</td>\n",
       "      <td>MGPQALLLLLSGTLVLTQPWARPHSLRYFYTAVSRPSRRDPRFSVV...</td>\n",
       "      <td>IPD-MHC:SLA08518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      species_prefix gene allele  \\\n",
       "14267            SLA    7  02:01   \n",
       "\n",
       "                                                     seq                id  \n",
       "14267  MGPQALLLLLSGTLVLTQPWARPHSLRYFYTAVSRPSRRDPRFSVV...  IPD-MHC:SLA08518  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.seq.str.len() > 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_onehot = np.zeros((n_samples, n_labels), dtype=\"int32\")\n",
    "for row_idx, label_idx in enumerate(encoded_labels):\n",
    "    encoded_labels_onehot[row_idx, label_idx] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Popy': 0,\n",
       " 'SLA': 1,\n",
       " 'DLA': 2,\n",
       " 'Pipi': 3,\n",
       " 'Sasa': 4,\n",
       " 'Sage': 5,\n",
       " 'Papa': 6,\n",
       " 'Lero': 7,\n",
       " 'Maar': 8,\n",
       " 'Cemi': 9,\n",
       " 'Atbe': 10,\n",
       " 'Paan': 11,\n",
       " 'Ovar': 12,\n",
       " 'Pacy': 13,\n",
       " 'Chae': 14,\n",
       " 'Patr': 15,\n",
       " 'Eqca': 16,\n",
       " 'Atfu': 17,\n",
       " 'Caja': 18,\n",
       " 'HLA': 19,\n",
       " 'Saoe': 20,\n",
       " 'Mafa': 21,\n",
       " 'BoLA': 22,\n",
       " 'Sasc': 23,\n",
       " 'Hyla': 24,\n",
       " 'Safu': 25,\n",
       " 'Onmy': 26,\n",
       " 'H-2': 27,\n",
       " 'Mane': 28,\n",
       " 'Paha': 29,\n",
       " 'Mamu': 30,\n",
       " 'Samy': 31,\n",
       " 'Gogo': 32}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 19, 19, ..., 27, 27, 27])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x102e0d490>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, TensorDataset, DataLoader\n",
    "from torch.optim import Adam \n",
    "\n",
    "class SpeciesClassifier(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_tokens,\n",
    "            n_labels,\n",
    "            embedding_dim=32,\n",
    "            lstm_hidden_dim=128,\n",
    "            batch_size=64,\n",
    "            lr=1e-3):\n",
    "        super(SpeciesClassifier, self).__init__()\n",
    "        self.n_tokens = n_tokens\n",
    "        self.n_labels = n_labels\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lstm_hidden_dim = lstm_hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embeddings = nn.Embedding(n_tokens, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, lstm_hidden_dim)\n",
    "        self.linear_classify = nn.Linear(lstm_hidden_dim, n_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, max_len = x.size()\n",
    "        embedded = self.embeddings(x)        \n",
    "        lstm_out, _  = self.lstm(embedded)\n",
    "        label_weights = self.linear_classify(lstm_out[:, -1, :].view(batch_size, -1))\n",
    "        label_scores = F.log_softmax(label_weights, dim=1)\n",
    "        return label_scores\n",
    "    \n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        logs = {'train_loss': loss}\n",
    "        return {'loss': loss, 'log': logs}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        return {'val_loss': loss}\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.X = torch.from_numpy(encoded_sequences).long()\n",
    "        self.y = torch.from_numpy(encoded_labels).long()\n",
    "        self.dataset = TensorDataset(self.X, self.y)\n",
    "        self.n_total = len(self.X)\n",
    "        self.n_train = int(0.66 * self.n_total)\n",
    "        self.n_val = int(0.1 * self.n_total)\n",
    "        self.n_test = (self.n_total - self.n_train) - self.n_val\n",
    "        (self.dataset_train, self.dataset_val, self.dataset_val) = \\\n",
    "            random_split(self.dataset, [self.n_train, self.n_val, self.n_test])\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, num_workers=2, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val, num_workers=2, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        print(\"LR\", self.lr)\n",
    "        optimizer = Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "# train\n",
    "model = SpeciesClassifier(n_tokens, n_labels)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = pl.Trainer(auto_lr_find=True, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type      | Params\n",
      "----------------------------------------------\n",
      "0 | embeddings      | Embedding | 768   \n",
      "1 | lstm            | LSTM      | 82 K  \n",
      "2 | linear_classify | Linear    | 4 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/usr/local/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94d15d8215294d7d9e9c01719bf6f0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Finding best initial lr', style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving latest checkpoint..\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Matplotlib is currently using module://ipykernel.pylab.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LR 0.00478630092322638\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run learning rate finder\n",
    "lr_finder = trainer.lr_find(model)\n",
    "\n",
    "# Results can be found in\n",
    "lr_finder.results\n",
    "\n",
    "# Plot with\n",
    "fig = lr_finder.plot(suggest=True)\n",
    "fig.show()\n",
    "\n",
    "# Pick point based on plot, or get suggestion\n",
    "new_lr = lr_finder.suggestion()\n",
    "print(\"Best LR\", new_lr)\n",
    "# update hparams of the model\n",
    "model.lr = new_lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name            | Type      | Params\n",
      "----------------------------------------------\n",
      "0 | embeddings      | Embedding | 768   \n",
      "1 | lstm            | LSTM      | 82 K  \n",
      "2 | linear_classify | Linear    | 4 K   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.00478630092322638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early due to diverging loss.\n",
      "/usr/local/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:37: UserWarning: You're resuming from a checkpoint that ended mid-epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint. \n",
      "  warnings.warn(*args, **kwargs)\n",
      "Failed to compute suggesting for `lr`. There might not be enough points.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/lr_finder.py\", line 352, in suggestion\n",
      "    min_grad = np.gradient(loss).argmin()\n",
      "  File \"<__array_function__ internals>\", line 6, in gradient\n",
      "  File \"/usr/local/lib/python3.7/site-packages/numpy/lib/function_base.py\", line 1042, in gradient\n",
      "    \"Shape of array too small to calculate a numerical gradient, \"\n",
      "ValueError: Shape of array too small to calculate a numerical gradient, at least (edge_order + 1) elements are required.\n",
      "Learning rate set to None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-45d4afebefac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/states.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mentering\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# The INTERRUPTED state can be set inside the run function. To indicate that run was interrupted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCPUBackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1083\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1084\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pytorch_lightning/accelerators/cpu_backend.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# CHOOSE OPTIMIZER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# allow for lr schedulers as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedulers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_frequencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pytorch_lightning/trainer/optimizers.py\u001b[0m in \u001b[0;36minit_optimizers\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     ) -> Tuple[List, List, List]:\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moptim_conf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptim_conf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-70-0e064357e7dc>\u001b[0m in \u001b[0;36mconfigure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LR\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad)\u001b[0m\n\u001b[1;32m     30\u001b[0m     def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n\u001b[1;32m     31\u001b[0m                  weight_decay=0, amsgrad=False):\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid learning rate: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<=' not supported between instances of 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 403, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 33])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(X_tensor[:32, :]).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003311311214825908"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
